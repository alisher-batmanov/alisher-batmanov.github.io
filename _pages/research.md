---
layout: archive
title: "Research"
nav_order: 1
permalink: /research/
author_profile: true
---

{% include base_path %}



## Working Papers

- <span style="color:#001f3d"><strong>“Mental Models, Social Learning and Statistical Discrimination: A Laboratory Study”</strong></span> <span id="mental-models-toggle" style="cursor: pointer; text-decoration: underline;" onclick="document.getElementById('mental-models-abstract').style.display='block'; this.style.display='none';"><strong>(Abstract)</strong></span>  
  <div id="mental-models-abstract" style="display: none; margin-top: 0.4em;">
    <strong>Abstract:</strong> In economic decision-making, individuals often rely on subjective representation of the environment to process information and make inferences. Using a laboratory experiment, I investigate how such mental models transform when people are exposed to the evaluations of others, particularly in scenarios where one or more parties may adopt an incorrect or misspecified model. Participants face a hiring task where their goal is to choose a worker with higher ability by integrating a noisy education signal with prior group information. The design of treatment conditions varies subjects' exposure to choices by another participant, using one group to present evaluations closely aligned with the theoretical Bayesian benchmark and another to expose subjects to evaluations consistent with signal neglect. I find that exposure to optimal behavior improves decision quality, with treated subjects making up to 22 percentage points more optimal choices. However, many participants appear to imitate others' decisions without internalizing the correct decision rule, leading to mislearning once the primitives of the environment change. Using a diagnostic treatment, I find that helping subjects recognize the optimality of others' choices only partially improves their decisions. Conversely, exposure to suboptimal choices has a weaker and statistically insignificant negative effect, with lower confidence in one’s choices strongly associated with following others’ suboptimal actions. These findings highlight the dual role of social learning: while it can enhance decision-making, it also fosters mechanical imitation that fails to generalize beyond the observed context.
  </div>
  Presented at: ESA North American Meeting (Columbus, 2024), Behavioral & Experimental Economics Student Conference (Caltech, 2024), Los Angeles Experiments Conference Poster (Caltech, 2024)
<!--   See others’ optimal choices: / learn by imitating, but / not when environment changes  -->
<div style="margin-top: 1.5em;"></div>


- <span style="color:#001f3d"><strong><a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5192345#" target="_blank" style="color:#001f3d; text-decoration: none;">“Beliefs and Demand for Mental Health Services Among University Students”</a></strong></span> <span id="mental-health-toggle" style="cursor: pointer; text-decoration: underline;" onclick="document.getElementById('mental-health-abstract').style.display='block'; this.style.display='none';"><strong>(Abstract)</strong></span>
  <div>(with Ida Grigoryeva, Bruno Calderon, Roberto González and Alejandro Guardiola Ramires)</div>
  <div><em>R&amp;R at Journal of Development Economics</em></div>
  <div id="mental-health-abstract" style="display: none; margin-top: 0.4em;">
     <strong>Abstract:</strong> This paper investigates the role of beliefs and stigma in shaping students’ use of professional mental health services at a large private university in Mexico, where supply-side barriers are minimal and services are readily accessible. In an online experiment with 680 students, we estimate a large treatment gap with nearly 50% of students in distress not receiving professional mental health support despite a high level of awareness and perceived effectiveness. We document stigmatized beliefs and misconceptions correlated with the treatment gap. For example, three-quarters of students incorrectly believe that those in distress perform worse academically, and many underestimate how common therapy use is among their peers. To correct inaccurate beliefs, we implement an information intervention and find that it increases students’ willingness to share on-campus mental health resources with peers and encourages them to recommend these resources when advising a friend in distress. However, we also find that it lowers their willingness to pay for external services, suggesting a potential substitution effect from private therapy to free on-campus resources.
  </div>
  Presented at: Advances with Field Experiments Conference (UChicago, 2025)*, NHH Field Experiments Conference (Bergen, 2024), Behavioral & Experimental Economics Student Conference (Caltech, 2023)
<!--   Students don’t go to therapy, why? / Unlikely inaccurate beliefs: / info treatment decreases demand  -->
<div style="margin-top: 1.5em;"></div>


- <span style="color:#001f3d"><strong><a href="https://docs.iza.org/dp16912.pdf" target="_blank" style="color:#001f3d; text-decoration: none;">“Mass Reproducibility and Replicability: A New Hope"</a></strong></span>  <span id="replicability-toggle" style="cursor: pointer; text-decoration: underline;" onclick="document.getElementById('replicability-abstract').style.display='block'; this.style.display='none';"><strong>(Abstract)</strong></span>
  <div>(with Abel Brodeur, Derek Mikola, Nikolai Cook, et al.)</div>
  <div><em>R&amp;R at Nature</em></div>
  <div id="replicability-abstract" style="display: none; margin-top: 0.4em;">
    <strong>Abstract:</strong> This study pushes our understanding of research reliability by reproducing and replicating claims from 110 papers in leading economic and political science journals. The analysis involves computational reproducibility checks and robustness assessments. It reveals several patterns. First, we uncover a high rate of fully computationally reproducible results (over 85%). Second, excluding minor issues like missing packages or broken pathways, we uncover coding errors for about 25% of studies, with some studies containing multiple errors. Third, we test the robustness of the results to 5,511 re-analyses. We find a robustness reproducibility of about 70%. Robustness reproducibility rates are relatively higher for re-analyses that introduce new data and lower for re-analyses that change the sample or the definition of the dependent variable. Fourth, 52% of re-analysis effect size estimates are smaller than the original published estimates and the average statistical significance of a re-analysis is 77% of the original. Lastly, we rely on six teams of researchers working independently to answer eight additional research questions on the determinants of robustness reproducibility. Most teams find a negative relationship between replicators' experience and reproducibility, while finding no relationship between reproducibility and the provision of intermediate or even raw data combined with the necessary cleaning codes.
  </div>  
<!--   Replicate 110 papers: / computational reproducibility 85% and / robustness reproducibility 70%  -->


## Work in Progress

- <span style="color:#333333"><strong>“Learning and Contingent Thinking: An Experiment”</strong></span> <span id="lct-toggle" style="cursor: pointer; text-decoration: underline;" onclick="document.getElementById('lct-abstract').style.display='block'; this.style.display='none';"><strong>(Abstract)</strong></span>  
  <div style="margin-top: 0.2em;"><em>(with Jeongbin Kim and Emanuel Vespa)</em></div>
  <div id="lct-abstract" style="display: none; margin-top: 0.4em;">
    <strong>Abstract:</strong> We study how individuals learn to make optimal decisions when doing so requires conditioning on payoff-relevant contingencies, as prescribed by theory. Using a laboratory experiment, we contrast learning across two environments: one in which irrelevant contingencies are excluded by design, and another in which they occur with positive probability but never affect outcomes. At baseline, 4 out of 5 subjects make a suboptimal choice. With experience and feedback, 71% of participants in the zero-probability environment switch to the optimal choice, compared to 49% in the setting where such contingencies remain present, indicating that engagement with payoff-irrelevant situations hinders learning. Participants in the zero-probability treatment are also more likely to report correct beliefs about the task environment and, conditional on holding accurate beliefs, are 15 percentage points more likely to make an optimal choice. These findings suggest that in many environments that necessitate reasoning through contingencies, not engaging with events that are irrelevant to the outcome can significantly ease learning and lead to optimal behavior.
  </div>
  Presented at: ESA North American Meeting (Tucson, 2025)*
<div style="margin-top: 1.5em;"></div>

- <span style="color:#001f3d"><strong>“Experiment on Narratives and Information in Persuasion”</strong></span>  
  <em>(with Bridget Galaty)</em>
  
  
<!-- 
## Upcoming Travel

- <span style="color:#333333"><strong>May 27-30, 2025 -- Behavioral Economics Annual Meeting (BEAM), UC Berkeley</strong></span>  
<!-- - <span style="color:#333333"><strong>June 20-21, 2025 -- Summer Institute in Theory-Based Experiments (CTESS), Caltech</strong></span>   -->

<!-- 
- <span style="color:#333333"><strong>July 30 - Aug 1, 2025 -- AEA CSQIEP Mentoring Conference, Chicago</strong></span>  
<!-- - <span style="color:#333333"><strong>Aug 18-21, 2025 -- SITE, Stanford</strong></span>   -->

<!-- 
- <span style="color:#333333"><strong>Sep 13-17, 2025 -- The Chicago School in Experimental Economics (CSEE), The University of Chicago</strong></span>  
 -->

<!-- 
- <span style="color:#333333"><strong>Sep 18-19, 2025 -- Advances with Field Experiments (AFE) Conference, The University of Chicago</strong></span>  
 -->




  
<!-- 
## Work in Progress

- <span style="color:#001f3d"><strong>“Mental Models, Social Learning and Statistical Discrimination: A Laboratory Study”</strong></span>
  <details>
    <summary style="cursor: pointer;">
      <small style="font-weight: bold;">See others’ optimal choices: / learn by imitating, but / not when environment changes.</small>
    </summary>
    <small style="display: block; margin-top: 0.4em;">
    <strong>Abstract:</strong> In economic decision-making, individuals often rely on subjective representation of the environment to process information and make inferences. Using a laboratory experiment, we investigate how such mental models transform when people are exposed to the evaluations of others, particularly in scenarios where one or more parties may adopt an incorrect or misspecified model. Participants face a hiring task where their goal is to choose a worker with higher ability by integrating a noisy education signal with prior group information. The design of treatment conditions variably exposes subjects to choices by another participant, using one group to present evaluations closely aligned with the theoretical Bayesian benchmark and another to expose subjects to evaluations from a participant exhibiting conservatism bias (signal neglect). We find that exposure to conservative choices leads to higher incidence of suboptimal behavior relative to exposure to Bayesian choices. Using elicited confidence measures and a diagnostic treatment with detailed feedback, we explore potential mechanisms. The results suggest that not recognizing the optimality of others’ choices partially hinders social learning. In addition, lower confidence in one’s choices is strongly associated with following others’ suboptimal actions, but not with learning from the optimal ones.
    </small>
  </details>

- <span style="color:#001f3d"><strong>“Stigma, Beliefs and Demand for Mental Health Services Among University Students”</strong></span>  
  <small>(with Ida Grigoryeva, Bruno Calderon, Roberto González and Alejandro Guardiola Ramires)</small>
  <details>
    <summary style="cursor: pointer;">
      <small style="font-weight: bold;">Students don’t go to therapy, why? / Unlikely inaccurate beliefs: / info treatment decreases demand.</small>
    </summary>
    <small style="display: block; margin-top: 0.4em;">
       <strong>Abstract:</strong> This paper investigates the role of beliefs and stigma in shaping students’ use of professional mental health services at a large private university in Mexico, where supply-side barriers are minimal and services are readily accessible. In an online experiment with 680 students, we estimate a large treatment gap with nearly 50% of students in distress not receiving professional mental health support despite a high level of awareness and perceived effectiveness. We document stigmatized beliefs and misconceptions correlated with the treatment gap. For example, three-quarters of students incorrectly believe that those in distress perform worse academically, and many underestimate how common therapy use is among their peers. To correct inaccurate beliefs, we implement an information intervention and find that it increases students’ willingness to share on-campus mental health resources with peers and encourages them to recommend these resources when advising a friend in distress. However, we also find that it lowers their willingness to pay for external services, suggesting a potential substitution effect from private therapy to free on-campus resources
    </small>
  </details>

## Working Papers

- <strong>"Mass Reproducibility and Replicability: A New Hope" (2024)</strong>  
  <small>(with Abel Brodeur, Derek Mikola, Nikolai Cook, et al.)</small>
  <details>
    <summary style="cursor: pointer;">
      <small style="font-weight: bold;">Replicate results of 110 papers: / computational reproducibility 85% and / robustness reproducibility 70% from 5,511 re-analyses.</small>
    </summary>
    <small style="display: block; margin-top: 0.4em;">
       <strong>Abstract:</strong> This study pushes our understanding of research reliability by reproducing and replicating claims from 110 papers in leading economic and political science journals. The analysis involves computational reproducibility checks and robustness assessments. It reveals several patterns. First, we uncover a high rate of fully computationally reproducible results (over 85%). Second, excluding minor issues like missing packages or broken pathways, we uncover coding errors for about 25% of studies, with some studies containing multiple errors. Third, we test the robustness of the results to 5,511 re-analyses. We find a robustness reproducibility of about 70%. Robustness reproducibility rates are relatively higher for re-analyses that introduce new data and lower for re-analyses that change the sample or the definition of the dependent variable. Fourth, 52% of re-analysis effect size estimates are smaller than the original published estimates and the average statistical significance of a re-analysis is 77% of the original. Lastly, we rely on six teams of researchers working independently to answer eight additional research questions on the determinants of robustness reproducibility. Most teams find a negative relationship between replicators' experience and reproducibility, while finding no relationship between reproducibility and the provision of intermediate or even raw data combined with the necessary cleaning codes.
    </small>
  </details>
 -->




